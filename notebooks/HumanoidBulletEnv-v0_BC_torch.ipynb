{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d6804b1",
   "metadata": {},
   "source": [
    "# Using Behavior Cloning to Play HumanoidBulletEnv-v0\n",
    "\n",
    "PyTorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e88317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import imp\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import pandas as pd\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "imp.reload(logging)\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        stream=sys.stdout, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b2f22",
   "metadata": {},
   "source": [
    "#### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3888b805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:02:33 [INFO] env: <HumanoidBulletEnv<HumanoidBulletEnv-v0>>\n",
      "00:02:33 [INFO] action_space: Box(-1.0, 1.0, (17,), float32)\n",
      "00:02:33 [INFO] observation_space: Box(-inf, inf, (44,), float32)\n",
      "00:02:33 [INFO] reward_range: (-inf, inf)\n",
      "00:02:33 [INFO] metadata: {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}\n",
      "00:02:33 [INFO] _max_episode_steps: 1000\n",
      "00:02:33 [INFO] _elapsed_steps: None\n",
      "00:02:33 [INFO] id: HumanoidBulletEnv-v0\n",
      "00:02:33 [INFO] entry_point: pybullet_envs.gym_locomotion_envs:HumanoidBulletEnv\n",
      "00:02:33 [INFO] reward_threshold: None\n",
      "00:02:33 [INFO] nondeterministic: False\n",
      "00:02:33 [INFO] max_episode_steps: 1000\n",
      "00:02:33 [INFO] _kwargs: {}\n",
      "00:02:33 [INFO] _env_name: HumanoidBulletEnv\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"HumanoidBulletEnv-v0\")\n",
    "env.seed(0)\n",
    "for key in vars(env):\n",
    "    logging.info('%s: %s', key, vars(env)[key])\n",
    "for key in vars(env.spec):\n",
    "    logging.info('%s: %s', key, vars(env.spec)[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41278ac",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "193a61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from expertagent import ExpertAgent\n",
    "expert_agent = ExpertAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "269db7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAReplayer:\n",
    "    def __init__(self):\n",
    "        self.fields = ['state', 'action']\n",
    "        self.data = {field: [] for field in self.fields}\n",
    "        self.memory = pd.DataFrame()\n",
    "\n",
    "    def store(self, *args):\n",
    "        for field, arg in zip(self.fields, args):\n",
    "            self.data[field].append(arg)\n",
    "\n",
    "    def sample(self, size=None):\n",
    "        if len(self.memory) < len(self.data[self.fields[0]]):\n",
    "            self.memory = pd.DataFrame(self.data, columns=self.fields)\n",
    "        if size is None:\n",
    "            indices = self.memory.index\n",
    "        else:\n",
    "            indices = np.random.choice(self.memory.index, size=size)\n",
    "        return (np.stack(self.memory.loc[indices, field]) for field in\n",
    "                self.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174c627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCAgent:\n",
    "    def __init__(self, env, expert_agent):\n",
    "        self.expert_agent = expert_agent\n",
    "\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "\n",
    "        self.net = self.build_net(input_size=state_dim,\n",
    "                hidden_sizes=[256, 128], output_size=action_dim)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.net.parameters())\n",
    "\n",
    "    def build_net(self, input_size, hidden_sizes, output_size=1,\n",
    "            output_activator=None):\n",
    "        layers = []\n",
    "        for input_size, output_size in zip(\n",
    "                [input_size,] + hidden_sizes, hidden_sizes + [output_size,]):\n",
    "            layers.append(nn.Linear(input_size, output_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers = layers[:-1]\n",
    "        if output_activator:\n",
    "            layers.append(output_activator)\n",
    "        net = nn.Sequential(*layers)\n",
    "        return net\n",
    "\n",
    "    def reset(self, mode=None):\n",
    "        self.mode = mode\n",
    "        if self.mode == 'expert':\n",
    "            self.expert_agent.reset(mode)\n",
    "            self.expert_replayer = SAReplayer()\n",
    "\n",
    "    def step(self, observation, reward, done):\n",
    "        if self.mode == 'expert':\n",
    "            action = expert_agent.step(observation, reward, done)\n",
    "            self.expert_replayer.store(observation, action)\n",
    "        else:\n",
    "            state_tensor = torch.as_tensor(observation, dtype=torch.float\n",
    "                    ).unsqueeze(0)\n",
    "            action_tensor = self.net(state_tensor)\n",
    "            action = action_tensor.detach().numpy()[0]\n",
    "        return action\n",
    "\n",
    "    def close(self):\n",
    "        if self.mode == 'expert':\n",
    "            self.expert_agent.close()\n",
    "            for _ in range(10):\n",
    "                self.learn()\n",
    "\n",
    "    def learn(self):\n",
    "        states, actions = self.expert_replayer.sample(1024)\n",
    "        state_tensor = torch.as_tensor(states, dtype=torch.float)\n",
    "        action_tensor = torch.as_tensor(actions, dtype=torch.float)\n",
    "\n",
    "        pred_tensor = self.net(state_tensor)\n",
    "        loss_tensor = self.loss(pred_tensor, action_tensor)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss_tensor.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "agent = BCAgent(env, expert_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ee87f6e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:02:33 [INFO] ==== expert ====\n",
      "00:02:44 [DEBUG] expert episode 0: reward = 3636.76, steps = 1000\n",
      "00:02:55 [DEBUG] expert episode 1: reward = 3584.22, steps = 1000\n",
      "00:03:06 [DEBUG] expert episode 2: reward = 3582.76, steps = 1000\n",
      "00:03:17 [DEBUG] expert episode 3: reward = 3641.40, steps = 1000\n",
      "00:03:28 [DEBUG] expert episode 4: reward = 3581.67, steps = 1000\n",
      "00:03:39 [DEBUG] expert episode 5: reward = 3597.90, steps = 1000\n",
      "00:03:50 [DEBUG] expert episode 6: reward = 3576.58, steps = 1000\n",
      "00:04:01 [DEBUG] expert episode 7: reward = 3627.58, steps = 1000\n",
      "00:04:13 [DEBUG] expert episode 8: reward = 3585.69, steps = 1000\n",
      "00:04:24 [DEBUG] expert episode 9: reward = 3612.16, steps = 1000\n",
      "00:04:36 [DEBUG] expert episode 10: reward = 3534.30, steps = 1000\n",
      "00:04:50 [DEBUG] expert episode 11: reward = 3621.15, steps = 1000\n",
      "00:04:51 [DEBUG] expert episode 12: reward = 19.42, steps = 67\n",
      "00:05:05 [DEBUG] expert episode 13: reward = 3595.52, steps = 1000\n",
      "00:05:17 [DEBUG] expert episode 14: reward = 3612.40, steps = 1000\n",
      "00:05:28 [DEBUG] expert episode 15: reward = 3607.04, steps = 1000\n",
      "00:05:39 [DEBUG] expert episode 16: reward = 3626.01, steps = 1000\n",
      "00:05:50 [DEBUG] expert episode 17: reward = 3628.98, steps = 1000\n",
      "00:05:50 [DEBUG] expert episode 18: reward = -3.76, steps = 22\n",
      "00:06:02 [DEBUG] expert episode 19: reward = 3621.28, steps = 1000\n",
      "00:06:12 [DEBUG] expert episode 20: reward = 3562.74, steps = 1000\n",
      "00:06:13 [DEBUG] expert episode 21: reward = -10.92, steps = 29\n",
      "00:06:24 [DEBUG] expert episode 22: reward = 3626.82, steps = 1000\n",
      "00:06:25 [DEBUG] expert episode 23: reward = 7.51, steps = 49\n",
      "00:06:35 [DEBUG] expert episode 24: reward = 3637.81, steps = 1000\n",
      "00:06:47 [DEBUG] expert episode 25: reward = 3594.70, steps = 1000\n",
      "00:06:58 [DEBUG] expert episode 26: reward = 3583.24, steps = 1000\n",
      "00:07:08 [DEBUG] expert episode 27: reward = 3637.90, steps = 1000\n",
      "00:07:09 [DEBUG] expert episode 28: reward = 61.78, steps = 79\n",
      "00:07:20 [DEBUG] expert episode 29: reward = 3604.58, steps = 1000\n",
      "00:07:31 [DEBUG] expert episode 30: reward = 3611.99, steps = 1000\n",
      "00:07:42 [DEBUG] expert episode 31: reward = 3638.35, steps = 1000\n",
      "00:07:52 [DEBUG] expert episode 32: reward = 3584.59, steps = 1000\n",
      "00:08:03 [DEBUG] expert episode 33: reward = 3612.45, steps = 1000\n",
      "00:08:14 [DEBUG] expert episode 34: reward = 3639.87, steps = 1000\n",
      "00:08:15 [DEBUG] expert episode 35: reward = -4.10, steps = 37\n",
      "00:08:26 [DEBUG] expert episode 36: reward = 3610.86, steps = 1000\n",
      "00:08:36 [DEBUG] expert episode 37: reward = 3627.18, steps = 1000\n",
      "00:08:47 [DEBUG] expert episode 38: reward = 3625.77, steps = 1000\n",
      "00:08:58 [DEBUG] expert episode 39: reward = 3602.48, steps = 1000\n",
      "00:09:08 [DEBUG] expert episode 40: reward = 3513.75, steps = 1000\n",
      "00:09:19 [DEBUG] expert episode 41: reward = 3616.72, steps = 1000\n",
      "00:09:20 [DEBUG] expert episode 42: reward = 3.92, steps = 33\n",
      "00:09:31 [DEBUG] expert episode 43: reward = 3610.82, steps = 1000\n",
      "00:09:42 [DEBUG] expert episode 44: reward = 3590.80, steps = 1000\n",
      "00:09:53 [DEBUG] expert episode 45: reward = 3598.02, steps = 1000\n",
      "00:10:04 [DEBUG] expert episode 46: reward = 3584.06, steps = 1000\n",
      "00:10:16 [DEBUG] expert episode 47: reward = 3631.53, steps = 1000\n",
      "00:10:27 [DEBUG] expert episode 48: reward = 3635.42, steps = 1000\n",
      "00:10:38 [DEBUG] expert episode 49: reward = 3614.20, steps = 1000\n",
      "00:10:49 [DEBUG] expert episode 50: reward = 3616.14, steps = 1000\n",
      "00:11:00 [DEBUG] expert episode 51: reward = 3561.36, steps = 1000\n",
      "00:11:11 [DEBUG] expert episode 52: reward = 3597.01, steps = 1000\n",
      "00:11:22 [DEBUG] expert episode 53: reward = 3599.24, steps = 1000\n",
      "00:11:33 [DEBUG] expert episode 54: reward = 3619.03, steps = 1000\n",
      "00:11:44 [DEBUG] expert episode 55: reward = 3627.83, steps = 1000\n",
      "00:11:55 [DEBUG] expert episode 56: reward = 3626.63, steps = 1000\n",
      "00:12:06 [DEBUG] expert episode 57: reward = 3595.42, steps = 1000\n",
      "00:12:18 [DEBUG] expert episode 58: reward = 3608.29, steps = 1000\n",
      "00:12:19 [DEBUG] expert episode 59: reward = 32.05, steps = 86\n",
      "00:12:30 [DEBUG] expert episode 60: reward = 3555.76, steps = 1000\n",
      "00:12:41 [DEBUG] expert episode 61: reward = 3608.93, steps = 1000\n",
      "00:12:52 [DEBUG] expert episode 62: reward = 3618.58, steps = 1000\n",
      "00:13:03 [DEBUG] expert episode 63: reward = 3628.57, steps = 1000\n",
      "00:13:14 [DEBUG] expert episode 64: reward = 3638.96, steps = 1000\n",
      "00:13:25 [DEBUG] expert episode 65: reward = 3617.05, steps = 1000\n",
      "00:13:36 [DEBUG] expert episode 66: reward = 3598.20, steps = 1000\n",
      "00:13:47 [DEBUG] expert episode 67: reward = 3445.20, steps = 1000\n",
      "00:13:58 [DEBUG] expert episode 68: reward = 3611.77, steps = 1000\n",
      "00:14:09 [DEBUG] expert episode 69: reward = 3607.09, steps = 1000\n",
      "00:14:20 [DEBUG] expert episode 70: reward = 3621.62, steps = 1000\n",
      "00:14:31 [DEBUG] expert episode 71: reward = 3602.90, steps = 1000\n",
      "00:14:42 [DEBUG] expert episode 72: reward = 3530.39, steps = 1000\n",
      "00:14:53 [DEBUG] expert episode 73: reward = 3603.60, steps = 1000\n",
      "00:14:54 [DEBUG] expert episode 74: reward = 50.15, steps = 77\n",
      "00:15:05 [DEBUG] expert episode 75: reward = 3592.32, steps = 1000\n",
      "00:15:16 [DEBUG] expert episode 76: reward = 3620.81, steps = 1000\n",
      "00:15:27 [DEBUG] expert episode 77: reward = 3611.51, steps = 1000\n",
      "00:15:38 [DEBUG] expert episode 78: reward = 3599.40, steps = 1000\n",
      "00:15:49 [DEBUG] expert episode 79: reward = 3632.42, steps = 1000\n",
      "00:16:00 [DEBUG] expert episode 80: reward = 3602.78, steps = 1000\n",
      "00:16:11 [DEBUG] expert episode 81: reward = 3646.05, steps = 1000\n",
      "00:16:22 [DEBUG] expert episode 82: reward = 3609.66, steps = 1000\n",
      "00:16:33 [DEBUG] expert episode 83: reward = 3606.29, steps = 1000\n",
      "00:16:45 [DEBUG] expert episode 84: reward = 3601.71, steps = 1000\n",
      "00:16:45 [DEBUG] expert episode 85: reward = -1.99, steps = 34\n",
      "00:16:56 [DEBUG] expert episode 86: reward = 3595.97, steps = 1000\n",
      "00:17:07 [DEBUG] expert episode 87: reward = 3620.52, steps = 1000\n",
      "00:17:18 [DEBUG] expert episode 88: reward = 3601.69, steps = 1000\n",
      "00:17:29 [DEBUG] expert episode 89: reward = 3594.18, steps = 1000\n",
      "00:17:40 [DEBUG] expert episode 90: reward = 3460.20, steps = 1000\n",
      "00:17:51 [DEBUG] expert episode 91: reward = 3619.25, steps = 1000\n",
      "00:18:02 [DEBUG] expert episode 92: reward = 3619.31, steps = 1000\n",
      "00:18:13 [DEBUG] expert episode 93: reward = 3417.55, steps = 1000\n",
      "00:18:25 [DEBUG] expert episode 94: reward = 3615.89, steps = 1000\n",
      "00:18:35 [DEBUG] expert episode 95: reward = 3591.82, steps = 1000\n",
      "00:18:46 [DEBUG] expert episode 96: reward = 3623.42, steps = 1000\n",
      "00:18:57 [DEBUG] expert episode 97: reward = 3614.98, steps = 1000\n",
      "00:19:08 [DEBUG] expert episode 98: reward = 3581.44, steps = 1000\n",
      "00:19:19 [DEBUG] expert episode 99: reward = 3579.60, steps = 1000\n",
      "00:19:19 [INFO] average expert episode reward = 3242.22 ± 1076.24\n",
      "00:19:19 [INFO] ==== test ====\n",
      "00:19:19 [DEBUG] test episode 0: reward = 109.70, steps = 55\n",
      "00:19:20 [DEBUG] test episode 1: reward = 93.02, steps = 51\n",
      "00:19:20 [DEBUG] test episode 2: reward = 75.39, steps = 65\n",
      "00:19:20 [DEBUG] test episode 3: reward = 102.55, steps = 54\n",
      "00:19:21 [DEBUG] test episode 4: reward = 86.29, steps = 45\n",
      "00:19:21 [DEBUG] test episode 5: reward = 88.55, steps = 49\n",
      "00:19:21 [DEBUG] test episode 6: reward = 99.22, steps = 54\n",
      "00:19:22 [DEBUG] test episode 7: reward = 99.89, steps = 51\n",
      "00:19:22 [DEBUG] test episode 8: reward = 97.62, steps = 55\n",
      "00:19:22 [DEBUG] test episode 9: reward = 105.89, steps = 56\n",
      "00:19:23 [DEBUG] test episode 10: reward = 82.77, steps = 46\n",
      "00:19:23 [DEBUG] test episode 11: reward = 110.19, steps = 64\n",
      "00:19:23 [DEBUG] test episode 12: reward = 61.94, steps = 51\n",
      "00:19:23 [DEBUG] test episode 13: reward = -6.18, steps = 24\n",
      "00:19:23 [DEBUG] test episode 14: reward = -1.88, steps = 18\n",
      "00:19:24 [DEBUG] test episode 15: reward = 78.00, steps = 46\n",
      "00:19:24 [DEBUG] test episode 16: reward = 110.00, steps = 62\n",
      "00:19:25 [DEBUG] test episode 17: reward = 156.67, steps = 90\n",
      "00:19:25 [DEBUG] test episode 18: reward = 107.54, steps = 56\n",
      "00:19:25 [DEBUG] test episode 19: reward = 68.56, steps = 60\n",
      "00:19:26 [DEBUG] test episode 20: reward = 93.51, steps = 50\n",
      "00:19:26 [DEBUG] test episode 21: reward = 85.47, steps = 48\n",
      "00:19:26 [DEBUG] test episode 22: reward = 95.60, steps = 53\n",
      "00:19:26 [DEBUG] test episode 23: reward = 96.80, steps = 52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:19:27 [DEBUG] test episode 24: reward = 107.31, steps = 67\n",
      "00:19:27 [DEBUG] test episode 25: reward = -48.03, steps = 20\n",
      "00:19:27 [DEBUG] test episode 26: reward = 84.42, steps = 47\n",
      "00:19:27 [DEBUG] test episode 27: reward = 87.39, steps = 49\n",
      "00:19:28 [DEBUG] test episode 28: reward = 76.35, steps = 40\n",
      "00:19:28 [DEBUG] test episode 29: reward = 88.63, steps = 47\n",
      "00:19:28 [DEBUG] test episode 30: reward = 44.59, steps = 38\n",
      "00:19:29 [DEBUG] test episode 31: reward = 121.62, steps = 68\n",
      "00:19:29 [DEBUG] test episode 32: reward = 90.10, steps = 47\n",
      "00:19:29 [DEBUG] test episode 33: reward = 79.29, steps = 43\n",
      "00:19:29 [DEBUG] test episode 34: reward = 103.21, steps = 53\n",
      "00:19:30 [DEBUG] test episode 35: reward = 87.00, steps = 48\n",
      "00:19:30 [DEBUG] test episode 36: reward = 176.82, steps = 87\n",
      "00:19:30 [DEBUG] test episode 37: reward = 82.27, steps = 43\n",
      "00:19:31 [DEBUG] test episode 38: reward = 86.58, steps = 46\n",
      "00:19:31 [DEBUG] test episode 39: reward = 105.01, steps = 84\n",
      "00:19:32 [DEBUG] test episode 40: reward = 118.70, steps = 75\n",
      "00:19:32 [DEBUG] test episode 41: reward = 117.10, steps = 58\n",
      "00:19:32 [DEBUG] test episode 42: reward = 129.58, steps = 101\n",
      "00:19:33 [DEBUG] test episode 43: reward = 133.95, steps = 67\n",
      "00:19:33 [DEBUG] test episode 44: reward = 4.78, steps = 35\n",
      "00:19:33 [DEBUG] test episode 45: reward = 139.70, steps = 75\n",
      "00:19:34 [DEBUG] test episode 46: reward = 121.71, steps = 65\n",
      "00:19:34 [DEBUG] test episode 47: reward = 129.57, steps = 75\n",
      "00:19:34 [DEBUG] test episode 48: reward = 47.75, steps = 42\n",
      "00:19:35 [DEBUG] test episode 49: reward = 57.99, steps = 47\n",
      "00:19:35 [DEBUG] test episode 50: reward = 96.12, steps = 56\n",
      "00:19:35 [DEBUG] test episode 51: reward = 78.11, steps = 45\n",
      "00:19:36 [DEBUG] test episode 52: reward = 97.64, steps = 58\n",
      "00:19:36 [DEBUG] test episode 53: reward = 164.66, steps = 76\n",
      "00:19:36 [DEBUG] test episode 54: reward = 11.92, steps = 24\n",
      "00:19:37 [DEBUG] test episode 55: reward = 140.69, steps = 92\n",
      "00:19:37 [DEBUG] test episode 56: reward = 123.83, steps = 60\n",
      "00:19:37 [DEBUG] test episode 57: reward = 130.41, steps = 72\n",
      "00:19:38 [DEBUG] test episode 58: reward = 99.72, steps = 52\n",
      "00:19:38 [DEBUG] test episode 59: reward = 77.24, steps = 61\n",
      "00:19:38 [DEBUG] test episode 60: reward = 105.45, steps = 56\n",
      "00:19:39 [DEBUG] test episode 61: reward = 104.27, steps = 57\n",
      "00:19:39 [DEBUG] test episode 62: reward = 123.68, steps = 86\n",
      "00:19:39 [DEBUG] test episode 63: reward = 22.04, steps = 50\n",
      "00:19:40 [DEBUG] test episode 64: reward = 86.43, steps = 47\n",
      "00:19:40 [DEBUG] test episode 65: reward = 104.10, steps = 57\n",
      "00:19:40 [DEBUG] test episode 66: reward = 113.42, steps = 56\n",
      "00:19:41 [DEBUG] test episode 67: reward = 105.19, steps = 58\n",
      "00:19:41 [DEBUG] test episode 68: reward = 150.21, steps = 99\n",
      "00:19:42 [DEBUG] test episode 69: reward = 55.50, steps = 54\n",
      "00:19:42 [DEBUG] test episode 70: reward = 75.83, steps = 40\n",
      "00:19:42 [DEBUG] test episode 71: reward = 96.37, steps = 71\n",
      "00:19:43 [DEBUG] test episode 72: reward = 91.95, steps = 80\n",
      "00:19:43 [DEBUG] test episode 73: reward = 154.82, steps = 75\n",
      "00:19:43 [DEBUG] test episode 74: reward = 127.42, steps = 82\n",
      "00:19:44 [DEBUG] test episode 75: reward = 87.33, steps = 66\n",
      "00:19:44 [DEBUG] test episode 76: reward = 83.82, steps = 44\n",
      "00:19:44 [DEBUG] test episode 77: reward = 84.53, steps = 64\n",
      "00:19:45 [DEBUG] test episode 78: reward = 93.76, steps = 49\n",
      "00:19:45 [DEBUG] test episode 79: reward = 71.93, steps = 41\n",
      "00:19:45 [DEBUG] test episode 80: reward = 64.22, steps = 46\n",
      "00:19:46 [DEBUG] test episode 81: reward = 97.67, steps = 53\n",
      "00:19:46 [DEBUG] test episode 82: reward = 78.11, steps = 44\n",
      "00:19:46 [DEBUG] test episode 83: reward = 85.58, steps = 45\n",
      "00:19:46 [DEBUG] test episode 84: reward = 87.09, steps = 62\n",
      "00:19:47 [DEBUG] test episode 85: reward = 154.37, steps = 88\n",
      "00:19:47 [DEBUG] test episode 86: reward = 100.67, steps = 51\n",
      "00:19:47 [DEBUG] test episode 87: reward = 91.01, steps = 49\n",
      "00:19:48 [DEBUG] test episode 88: reward = 95.02, steps = 52\n",
      "00:19:48 [DEBUG] test episode 89: reward = 98.97, steps = 56\n",
      "00:19:49 [DEBUG] test episode 90: reward = 132.47, steps = 75\n",
      "00:19:49 [DEBUG] test episode 91: reward = -2.26, steps = 43\n",
      "00:19:49 [DEBUG] test episode 92: reward = 78.13, steps = 42\n",
      "00:19:50 [DEBUG] test episode 93: reward = 136.35, steps = 92\n",
      "00:19:50 [DEBUG] test episode 94: reward = 129.32, steps = 74\n",
      "00:19:50 [DEBUG] test episode 95: reward = 122.33, steps = 64\n",
      "00:19:51 [DEBUG] test episode 96: reward = 84.55, steps = 46\n",
      "00:19:51 [DEBUG] test episode 97: reward = 183.28, steps = 100\n",
      "00:19:52 [DEBUG] test episode 98: reward = 110.42, steps = 61\n",
      "00:19:52 [DEBUG] test episode 99: reward = 81.12, steps = 45\n",
      "00:19:52 [INFO] average episode reward = 94.37 ± 37.49\n"
     ]
    }
   ],
   "source": [
    "def play_episode(env, agent, max_episode_steps=None, mode=None, render=False):\n",
    "    observation, reward, done = env.reset(), 0., False\n",
    "    agent.reset(mode=mode)\n",
    "    episode_reward, elapsed_steps = 0., 0\n",
    "    while True:\n",
    "        action = agent.step(observation, reward, done)\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        elapsed_steps += 1\n",
    "        if max_episode_steps and elapsed_steps >= max_episode_steps:\n",
    "            break\n",
    "    agent.close()\n",
    "    return episode_reward, elapsed_steps\n",
    "\n",
    "\n",
    "\n",
    "logging.info('==== expert ====')\n",
    "episode_rewards = []\n",
    "for episode in range(100):\n",
    "    episode_reward, elapsed_steps = play_episode(env, agent, mode='expert')\n",
    "    episode_rewards.append(episode_reward)\n",
    "    logging.debug('expert episode %d: reward = %.2f, steps = %d',\n",
    "            episode, episode_reward, elapsed_steps)\n",
    "logging.info('average expert episode reward = %.2f ± %.2f',\n",
    "        np.mean(episode_rewards), np.std(episode_rewards))\n",
    "\n",
    "\n",
    "logging.info('==== test ====')\n",
    "episode_rewards = []\n",
    "for episode in range(100):\n",
    "    episode_reward, elapsed_steps = play_episode(env, agent)\n",
    "    episode_rewards.append(episode_reward)\n",
    "    logging.debug('test episode %d: reward = %.2f, steps = %d',\n",
    "            episode, episode_reward, elapsed_steps)\n",
    "logging.info('average episode reward = %.2f ± %.2f',\n",
    "        np.mean(episode_rewards), np.std(episode_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9f600f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
