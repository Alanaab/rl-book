{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d6804b1",
   "metadata": {},
   "source": [
    "# Using Behavior Cloning to Play HumanoidBulletEnv-v0\n",
    "\n",
    "TensorFlow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e88317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import pandas as pd\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.random.set_seed(0)\n",
    "from tensorflow import keras\n",
    "from tensorflow import nn\n",
    "from tensorflow import optimizers\n",
    "from tensorflow import losses\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        stream=sys.stdout, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b2f22",
   "metadata": {},
   "source": [
    "#### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3888b805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:09:50 [INFO] env: <HumanoidBulletEnv<HumanoidBulletEnv-v0>>\n",
      "00:09:50 [INFO] action_space: Box(-1.0, 1.0, (17,), float32)\n",
      "00:09:50 [INFO] observation_space: Box(-inf, inf, (44,), float32)\n",
      "00:09:50 [INFO] reward_range: (-inf, inf)\n",
      "00:09:50 [INFO] metadata: {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}\n",
      "00:09:50 [INFO] _max_episode_steps: 1000\n",
      "00:09:50 [INFO] _elapsed_steps: None\n",
      "00:09:50 [INFO] id: HumanoidBulletEnv-v0\n",
      "00:09:50 [INFO] entry_point: pybullet_envs.gym_locomotion_envs:HumanoidBulletEnv\n",
      "00:09:50 [INFO] reward_threshold: None\n",
      "00:09:50 [INFO] nondeterministic: False\n",
      "00:09:50 [INFO] max_episode_steps: 1000\n",
      "00:09:50 [INFO] _kwargs: {}\n",
      "00:09:50 [INFO] _env_name: HumanoidBulletEnv\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"HumanoidBulletEnv-v0\")\n",
    "env.seed(0)\n",
    "for key in vars(env):\n",
    "    logging.info('%s: %s', key, vars(env)[key])\n",
    "for key in vars(env.spec):\n",
    "    logging.info('%s: %s', key, vars(env.spec)[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41278ac",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "193a61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from expertagent import ExpertAgent\n",
    "expert_agent = ExpertAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "269db7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAReplayer:\n",
    "    def __init__(self):\n",
    "        self.fields = ['state', 'action']\n",
    "        self.data = {field: [] for field in self.fields}\n",
    "        self.memory = pd.DataFrame()\n",
    "\n",
    "    def store(self, *args):\n",
    "        for field, arg in zip(self.fields, args):\n",
    "            self.data[field].append(arg)\n",
    "\n",
    "    def sample(self, size=None):\n",
    "        if len(self.memory) < len(self.data[self.fields[0]]):\n",
    "            self.memory = pd.DataFrame(self.data, columns=self.fields)\n",
    "        if size is None:\n",
    "            indices = self.memory.index\n",
    "        else:\n",
    "            indices = np.random.choice(self.memory.index, size=size)\n",
    "        return (np.stack(self.memory.loc[indices, field]) for field in\n",
    "                self.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174c627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCAgent:\n",
    "    def __init__(self, env, expert_agent):\n",
    "        self.expert_agent = expert_agent\n",
    "\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "\n",
    "        self.net = self.build_net(input_size=state_dim,\n",
    "                hidden_sizes=[256, 128], output_size=action_dim)\n",
    "\n",
    "    def build_net(self, input_size=None, hidden_sizes=None, output_size=1,\n",
    "                activation=nn.relu, output_activation=None,\n",
    "                loss=losses.mse, learning_rate=0.001):\n",
    "        model = keras.Sequential()\n",
    "        for hidden_size in hidden_sizes:\n",
    "            model.add(layers.Dense(units=hidden_size,\n",
    "                    activation=activation))\n",
    "        model.add(layers.Dense(units=output_size,\n",
    "                activation=output_activation))\n",
    "        optimizer = optimizers.Adam(learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=loss)\n",
    "        return model\n",
    "\n",
    "    def reset(self, mode=None):\n",
    "        self.mode = mode\n",
    "        if self.mode == 'expert':\n",
    "            self.expert_agent.reset(mode)\n",
    "            self.expert_replayer = SAReplayer()\n",
    "\n",
    "    def step(self, observation, reward, done):\n",
    "        if self.mode == 'expert':\n",
    "            action = expert_agent.step(observation, reward, done)\n",
    "            self.expert_replayer.store(observation, action)\n",
    "        else:\n",
    "            action = self.net(observation[np.newaxis])[0]\n",
    "        return action\n",
    "\n",
    "    def close(self):\n",
    "        if self.mode == 'expert':\n",
    "            self.expert_agent.close()\n",
    "            for _ in range(10):\n",
    "                self.learn()\n",
    "\n",
    "    def learn(self):\n",
    "        states, actions = self.expert_replayer.sample(1024)\n",
    "        self.net.fit(states, actions, verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "agent = BCAgent(env, expert_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ee87f6e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:09:52 [INFO] ==== expert ====\n",
      "00:10:07 [DEBUG] expert episode 0: reward = 3636.76, steps = 1000\n",
      "00:10:19 [DEBUG] expert episode 1: reward = 3584.22, steps = 1000\n",
      "00:10:31 [DEBUG] expert episode 2: reward = 3582.76, steps = 1000\n",
      "00:10:43 [DEBUG] expert episode 3: reward = 3641.40, steps = 1000\n",
      "00:10:55 [DEBUG] expert episode 4: reward = 3581.67, steps = 1000\n",
      "00:11:07 [DEBUG] expert episode 5: reward = 3597.90, steps = 1000\n",
      "00:11:19 [DEBUG] expert episode 6: reward = 3576.58, steps = 1000\n",
      "00:11:32 [DEBUG] expert episode 7: reward = 3627.58, steps = 1000\n",
      "00:11:44 [DEBUG] expert episode 8: reward = 3585.69, steps = 1000\n",
      "00:11:57 [DEBUG] expert episode 9: reward = 3612.16, steps = 1000\n",
      "00:12:09 [DEBUG] expert episode 10: reward = 3534.30, steps = 1000\n",
      "00:12:21 [DEBUG] expert episode 11: reward = 3621.15, steps = 1000\n",
      "00:12:23 [DEBUG] expert episode 12: reward = 19.42, steps = 67\n",
      "00:12:36 [DEBUG] expert episode 13: reward = 3595.52, steps = 1000\n",
      "00:12:48 [DEBUG] expert episode 14: reward = 3612.40, steps = 1000\n",
      "00:13:00 [DEBUG] expert episode 15: reward = 3607.04, steps = 1000\n",
      "00:13:12 [DEBUG] expert episode 16: reward = 3626.01, steps = 1000\n",
      "00:13:24 [DEBUG] expert episode 17: reward = 3628.98, steps = 1000\n",
      "00:13:26 [DEBUG] expert episode 18: reward = -3.76, steps = 22\n",
      "00:13:38 [DEBUG] expert episode 19: reward = 3621.28, steps = 1000\n",
      "00:13:50 [DEBUG] expert episode 20: reward = 3562.74, steps = 1000\n",
      "00:13:51 [DEBUG] expert episode 21: reward = -10.92, steps = 29\n",
      "00:14:04 [DEBUG] expert episode 22: reward = 3626.82, steps = 1000\n",
      "00:14:05 [DEBUG] expert episode 23: reward = 7.51, steps = 49\n",
      "00:14:17 [DEBUG] expert episode 24: reward = 3637.81, steps = 1000\n",
      "00:14:29 [DEBUG] expert episode 25: reward = 3594.70, steps = 1000\n",
      "00:14:42 [DEBUG] expert episode 26: reward = 3583.24, steps = 1000\n",
      "00:14:53 [DEBUG] expert episode 27: reward = 3637.90, steps = 1000\n",
      "00:14:56 [DEBUG] expert episode 28: reward = 61.78, steps = 79\n",
      "00:15:08 [DEBUG] expert episode 29: reward = 3604.58, steps = 1000\n",
      "00:15:20 [DEBUG] expert episode 30: reward = 3611.99, steps = 1000\n",
      "00:15:32 [DEBUG] expert episode 31: reward = 3638.35, steps = 1000\n",
      "00:15:44 [DEBUG] expert episode 32: reward = 3584.59, steps = 1000\n",
      "00:15:56 [DEBUG] expert episode 33: reward = 3612.45, steps = 1000\n",
      "00:16:08 [DEBUG] expert episode 34: reward = 3639.87, steps = 1000\n",
      "00:16:10 [DEBUG] expert episode 35: reward = -4.10, steps = 37\n",
      "00:16:22 [DEBUG] expert episode 36: reward = 3610.86, steps = 1000\n",
      "00:16:34 [DEBUG] expert episode 37: reward = 3627.18, steps = 1000\n",
      "00:16:46 [DEBUG] expert episode 38: reward = 3625.77, steps = 1000\n",
      "00:16:58 [DEBUG] expert episode 39: reward = 3602.48, steps = 1000\n",
      "00:17:10 [DEBUG] expert episode 40: reward = 3513.75, steps = 1000\n",
      "00:17:22 [DEBUG] expert episode 41: reward = 3616.72, steps = 1000\n",
      "00:17:24 [DEBUG] expert episode 42: reward = 3.92, steps = 33\n",
      "00:17:36 [DEBUG] expert episode 43: reward = 3610.82, steps = 1000\n",
      "00:17:48 [DEBUG] expert episode 44: reward = 3590.80, steps = 1000\n",
      "00:18:00 [DEBUG] expert episode 45: reward = 3598.02, steps = 1000\n",
      "00:18:12 [DEBUG] expert episode 46: reward = 3584.06, steps = 1000\n",
      "00:18:25 [DEBUG] expert episode 47: reward = 3631.53, steps = 1000\n",
      "00:18:37 [DEBUG] expert episode 48: reward = 3635.42, steps = 1000\n",
      "00:18:49 [DEBUG] expert episode 49: reward = 3614.20, steps = 1000\n",
      "00:19:01 [DEBUG] expert episode 50: reward = 3616.14, steps = 1000\n",
      "00:19:13 [DEBUG] expert episode 51: reward = 3561.36, steps = 1000\n",
      "00:19:24 [DEBUG] expert episode 52: reward = 3597.01, steps = 1000\n",
      "00:19:36 [DEBUG] expert episode 53: reward = 3599.24, steps = 1000\n",
      "00:19:48 [DEBUG] expert episode 54: reward = 3619.03, steps = 1000\n",
      "00:20:00 [DEBUG] expert episode 55: reward = 3627.83, steps = 1000\n",
      "00:20:12 [DEBUG] expert episode 56: reward = 3626.63, steps = 1000\n",
      "00:20:23 [DEBUG] expert episode 57: reward = 3595.42, steps = 1000\n",
      "00:20:35 [DEBUG] expert episode 58: reward = 3608.29, steps = 1000\n",
      "00:20:37 [DEBUG] expert episode 59: reward = 32.05, steps = 86\n",
      "00:20:50 [DEBUG] expert episode 60: reward = 3555.76, steps = 1000\n",
      "00:21:02 [DEBUG] expert episode 61: reward = 3608.93, steps = 1000\n",
      "00:21:13 [DEBUG] expert episode 62: reward = 3618.58, steps = 1000\n",
      "00:21:25 [DEBUG] expert episode 63: reward = 3628.57, steps = 1000\n",
      "00:21:37 [DEBUG] expert episode 64: reward = 3638.96, steps = 1000\n",
      "00:21:48 [DEBUG] expert episode 65: reward = 3617.05, steps = 1000\n",
      "00:22:00 [DEBUG] expert episode 66: reward = 3598.20, steps = 1000\n",
      "00:22:12 [DEBUG] expert episode 67: reward = 3445.20, steps = 1000\n",
      "00:22:23 [DEBUG] expert episode 68: reward = 3611.77, steps = 1000\n",
      "00:22:35 [DEBUG] expert episode 69: reward = 3607.09, steps = 1000\n",
      "00:22:47 [DEBUG] expert episode 70: reward = 3621.62, steps = 1000\n",
      "00:22:58 [DEBUG] expert episode 71: reward = 3602.90, steps = 1000\n",
      "00:23:10 [DEBUG] expert episode 72: reward = 3530.39, steps = 1000\n",
      "00:23:22 [DEBUG] expert episode 73: reward = 3603.60, steps = 1000\n",
      "00:23:24 [DEBUG] expert episode 74: reward = 50.15, steps = 77\n",
      "00:23:35 [DEBUG] expert episode 75: reward = 3592.32, steps = 1000\n",
      "00:23:47 [DEBUG] expert episode 76: reward = 3620.81, steps = 1000\n",
      "00:23:59 [DEBUG] expert episode 77: reward = 3611.51, steps = 1000\n",
      "00:24:10 [DEBUG] expert episode 78: reward = 3599.40, steps = 1000\n",
      "00:24:22 [DEBUG] expert episode 79: reward = 3632.42, steps = 1000\n",
      "00:24:34 [DEBUG] expert episode 80: reward = 3602.78, steps = 1000\n",
      "00:24:45 [DEBUG] expert episode 81: reward = 3646.05, steps = 1000\n",
      "00:24:57 [DEBUG] expert episode 82: reward = 3609.66, steps = 1000\n",
      "00:25:08 [DEBUG] expert episode 83: reward = 3606.29, steps = 1000\n",
      "00:25:20 [DEBUG] expert episode 84: reward = 3601.71, steps = 1000\n",
      "00:25:21 [DEBUG] expert episode 85: reward = -1.99, steps = 34\n",
      "00:25:33 [DEBUG] expert episode 86: reward = 3595.97, steps = 1000\n",
      "00:25:45 [DEBUG] expert episode 87: reward = 3620.52, steps = 1000\n",
      "00:25:57 [DEBUG] expert episode 88: reward = 3601.69, steps = 1000\n",
      "00:26:08 [DEBUG] expert episode 89: reward = 3594.18, steps = 1000\n",
      "00:26:20 [DEBUG] expert episode 90: reward = 3460.20, steps = 1000\n",
      "00:26:31 [DEBUG] expert episode 91: reward = 3619.25, steps = 1000\n",
      "00:26:43 [DEBUG] expert episode 92: reward = 3619.31, steps = 1000\n",
      "00:26:55 [DEBUG] expert episode 93: reward = 3417.55, steps = 1000\n",
      "00:27:06 [DEBUG] expert episode 94: reward = 3615.89, steps = 1000\n",
      "00:27:19 [DEBUG] expert episode 95: reward = 3591.82, steps = 1000\n",
      "00:27:31 [DEBUG] expert episode 96: reward = 3623.42, steps = 1000\n",
      "00:27:43 [DEBUG] expert episode 97: reward = 3614.98, steps = 1000\n",
      "00:27:54 [DEBUG] expert episode 98: reward = 3581.44, steps = 1000\n",
      "00:28:06 [DEBUG] expert episode 99: reward = 3579.60, steps = 1000\n",
      "00:28:06 [INFO] average expert episode reward = 3242.22 ± 1076.24\n",
      "00:28:06 [INFO] ==== test ====\n",
      "00:28:19 [DEBUG] test episode 0: reward = 3591.33, steps = 1000\n",
      "00:28:21 [DEBUG] test episode 1: reward = 274.67, steps = 124\n",
      "00:28:22 [DEBUG] test episode 2: reward = 74.02, steps = 60\n",
      "00:28:23 [DEBUG] test episode 3: reward = 169.85, steps = 87\n",
      "00:28:24 [DEBUG] test episode 4: reward = 234.87, steps = 111\n",
      "00:28:25 [DEBUG] test episode 5: reward = 58.16, steps = 43\n",
      "00:28:27 [DEBUG] test episode 6: reward = 350.45, steps = 154\n",
      "00:28:28 [DEBUG] test episode 7: reward = 149.80, steps = 84\n",
      "00:28:29 [DEBUG] test episode 8: reward = 119.60, steps = 76\n",
      "00:28:31 [DEBUG] test episode 9: reward = 292.74, steps = 132\n",
      "00:28:32 [DEBUG] test episode 10: reward = 155.64, steps = 85\n",
      "00:28:33 [DEBUG] test episode 11: reward = 90.13, steps = 72\n",
      "00:28:34 [DEBUG] test episode 12: reward = 71.91, steps = 57\n",
      "00:28:35 [DEBUG] test episode 13: reward = 30.54, steps = 48\n",
      "00:28:35 [DEBUG] test episode 14: reward = -13.14, steps = 18\n",
      "00:28:37 [DEBUG] test episode 15: reward = 227.48, steps = 115\n",
      "00:28:38 [DEBUG] test episode 16: reward = 118.88, steps = 81\n",
      "00:28:40 [DEBUG] test episode 17: reward = 113.25, steps = 81\n",
      "00:28:41 [DEBUG] test episode 18: reward = 179.90, steps = 99\n",
      "00:28:42 [DEBUG] test episode 19: reward = 97.85, steps = 74\n",
      "00:28:44 [DEBUG] test episode 20: reward = 365.21, steps = 155\n",
      "00:28:46 [DEBUG] test episode 21: reward = 147.99, steps = 93\n",
      "00:28:48 [DEBUG] test episode 22: reward = 359.69, steps = 172\n",
      "00:28:52 [DEBUG] test episode 23: reward = 625.16, steps = 258\n",
      "00:28:53 [DEBUG] test episode 24: reward = 99.68, steps = 72\n",
      "00:28:53 [DEBUG] test episode 25: reward = -6.33, steps = 24\n",
      "00:29:06 [DEBUG] test episode 26: reward = 3618.06, steps = 1000\n",
      "00:29:07 [DEBUG] test episode 27: reward = 96.28, steps = 64\n",
      "00:29:08 [DEBUG] test episode 28: reward = 124.48, steps = 70\n",
      "00:29:22 [DEBUG] test episode 29: reward = 3614.14, steps = 1000\n",
      "00:29:22 [DEBUG] test episode 30: reward = 46.95, steps = 43\n",
      "00:29:24 [DEBUG] test episode 31: reward = 132.25, steps = 95\n",
      "00:29:26 [DEBUG] test episode 32: reward = 499.32, steps = 195\n",
      "00:29:28 [DEBUG] test episode 33: reward = 380.34, steps = 158\n",
      "00:29:29 [DEBUG] test episode 34: reward = 84.36, steps = 59\n",
      "00:29:32 [DEBUG] test episode 35: reward = 404.09, steps = 174\n",
      "00:29:33 [DEBUG] test episode 36: reward = 128.64, steps = 80\n",
      "00:29:39 [DEBUG] test episode 37: reward = 1493.17, steps = 505\n",
      "00:29:40 [DEBUG] test episode 38: reward = 73.96, steps = 53\n",
      "00:29:41 [DEBUG] test episode 39: reward = 105.85, steps = 88\n",
      "00:29:42 [DEBUG] test episode 40: reward = 125.77, steps = 79\n",
      "00:29:43 [DEBUG] test episode 41: reward = 68.77, steps = 47\n",
      "00:29:44 [DEBUG] test episode 42: reward = 108.92, steps = 72\n",
      "00:29:50 [DEBUG] test episode 43: reward = 1190.28, steps = 423\n",
      "00:29:50 [DEBUG] test episode 44: reward = 63.65, steps = 62\n",
      "00:29:53 [DEBUG] test episode 45: reward = 422.49, steps = 172\n",
      "00:29:54 [DEBUG] test episode 46: reward = 154.40, steps = 94\n",
      "00:29:55 [DEBUG] test episode 47: reward = 147.87, steps = 100\n",
      "00:29:56 [DEBUG] test episode 48: reward = 41.67, steps = 50\n",
      "00:29:57 [DEBUG] test episode 49: reward = 75.09, steps = 65\n",
      "00:29:59 [DEBUG] test episode 50: reward = 246.53, steps = 133\n",
      "00:30:01 [DEBUG] test episode 51: reward = 345.61, steps = 156\n",
      "00:30:02 [DEBUG] test episode 52: reward = 108.76, steps = 74\n",
      "00:30:03 [DEBUG] test episode 53: reward = 158.85, steps = 91\n",
      "00:30:04 [DEBUG] test episode 54: reward = 31.78, steps = 56\n",
      "00:30:05 [DEBUG] test episode 55: reward = 150.05, steps = 83\n",
      "00:30:08 [DEBUG] test episode 56: reward = 467.94, steps = 193\n",
      "00:30:09 [DEBUG] test episode 57: reward = 102.30, steps = 84\n",
      "00:30:10 [DEBUG] test episode 58: reward = 144.92, steps = 81\n",
      "00:30:11 [DEBUG] test episode 59: reward = 68.53, steps = 67\n",
      "00:30:14 [DEBUG] test episode 60: reward = 498.84, steps = 211\n",
      "00:30:15 [DEBUG] test episode 61: reward = 182.17, steps = 105\n",
      "00:30:19 [DEBUG] test episode 62: reward = 693.49, steps = 267\n",
      "00:30:20 [DEBUG] test episode 63: reward = 69.82, steps = 78\n",
      "00:30:22 [DEBUG] test episode 64: reward = 443.87, steps = 179\n",
      "00:30:23 [DEBUG] test episode 65: reward = 143.08, steps = 98\n",
      "00:30:26 [DEBUG] test episode 66: reward = 484.11, steps = 191\n",
      "00:30:27 [DEBUG] test episode 67: reward = 152.25, steps = 87\n",
      "00:30:28 [DEBUG] test episode 68: reward = 113.86, steps = 75\n",
      "00:30:29 [DEBUG] test episode 69: reward = 47.38, steps = 52\n",
      "00:30:30 [DEBUG] test episode 70: reward = 160.67, steps = 83\n",
      "00:30:31 [DEBUG] test episode 71: reward = 88.66, steps = 66\n",
      "00:30:32 [DEBUG] test episode 72: reward = 112.63, steps = 77\n",
      "00:30:34 [DEBUG] test episode 73: reward = 310.11, steps = 144\n",
      "00:30:35 [DEBUG] test episode 74: reward = 79.71, steps = 82\n",
      "00:30:36 [DEBUG] test episode 75: reward = 84.37, steps = 81\n",
      "00:30:49 [DEBUG] test episode 76: reward = 3588.07, steps = 1000\n",
      "00:30:51 [DEBUG] test episode 77: reward = 147.01, steps = 98\n",
      "00:30:51 [DEBUG] test episode 78: reward = 89.59, steps = 59\n",
      "00:30:52 [DEBUG] test episode 79: reward = 73.18, steps = 53\n",
      "00:30:53 [DEBUG] test episode 80: reward = 80.16, steps = 70\n",
      "00:30:55 [DEBUG] test episode 81: reward = 281.17, steps = 139\n",
      "00:30:56 [DEBUG] test episode 82: reward = 84.49, steps = 64\n",
      "00:31:09 [DEBUG] test episode 83: reward = 3625.25, steps = 1000\n",
      "00:31:10 [DEBUG] test episode 84: reward = 89.55, steps = 73\n",
      "00:31:12 [DEBUG] test episode 85: reward = 247.93, steps = 139\n",
      "00:31:14 [DEBUG] test episode 86: reward = 384.44, steps = 160\n",
      "00:31:16 [DEBUG] test episode 87: reward = 219.08, steps = 111\n",
      "00:31:18 [DEBUG] test episode 88: reward = 452.34, steps = 188\n",
      "00:31:20 [DEBUG] test episode 89: reward = 386.57, steps = 163\n",
      "00:31:21 [DEBUG] test episode 90: reward = 129.46, steps = 85\n",
      "00:31:22 [DEBUG] test episode 91: reward = 32.67, steps = 49\n",
      "00:31:24 [DEBUG] test episode 92: reward = 313.64, steps = 135\n",
      "00:31:25 [DEBUG] test episode 93: reward = 96.43, steps = 73\n",
      "00:31:26 [DEBUG] test episode 94: reward = 103.55, steps = 92\n",
      "00:31:27 [DEBUG] test episode 95: reward = 214.60, steps = 108\n",
      "00:31:30 [DEBUG] test episode 96: reward = 503.47, steps = 205\n",
      "00:31:31 [DEBUG] test episode 97: reward = 199.30, steps = 104\n",
      "00:31:32 [DEBUG] test episode 98: reward = 125.53, steps = 80\n",
      "00:31:33 [DEBUG] test episode 99: reward = 125.81, steps = 72\n",
      "00:31:33 [INFO] average episode reward = 382.64 ± 770.74\n"
     ]
    }
   ],
   "source": [
    "def play_episode(env, agent, max_episode_steps=None, mode=None, render=False):\n",
    "    observation, reward, done = env.reset(), 0., False\n",
    "    agent.reset(mode=mode)\n",
    "    episode_reward, elapsed_steps = 0., 0\n",
    "    while True:\n",
    "        action = agent.step(observation, reward, done)\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        elapsed_steps += 1\n",
    "        if max_episode_steps and elapsed_steps >= max_episode_steps:\n",
    "            break\n",
    "    agent.close()\n",
    "    return episode_reward, elapsed_steps\n",
    "\n",
    "\n",
    "\n",
    "logging.info('==== expert ====')\n",
    "episode_rewards = []\n",
    "for episode in range(100):\n",
    "    episode_reward, elapsed_steps = play_episode(env, agent, mode='expert')\n",
    "    episode_rewards.append(episode_reward)\n",
    "    logging.debug('expert episode %d: reward = %.2f, steps = %d',\n",
    "            episode, episode_reward, elapsed_steps)\n",
    "logging.info('average expert episode reward = %.2f ± %.2f',\n",
    "        np.mean(episode_rewards), np.std(episode_rewards))\n",
    "\n",
    "\n",
    "logging.info('==== test ====')\n",
    "episode_rewards = []\n",
    "for episode in range(100):\n",
    "    episode_reward, elapsed_steps = play_episode(env, agent)\n",
    "    episode_rewards.append(episode_reward)\n",
    "    logging.debug('test episode %d: reward = %.2f, steps = %d',\n",
    "            episode, episode_reward, elapsed_steps)\n",
    "logging.info('average episode reward = %.2f ± %.2f',\n",
    "        np.mean(episode_rewards), np.std(episode_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9f600f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
